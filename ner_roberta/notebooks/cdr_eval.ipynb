{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e02b25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d846678",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e2f949",
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqeval.metrics import (\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    classification_report,\n",
    ")\n",
    "from seqeval.scheme import IOB2        # BIO/IOB2 tagging scheme\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10534b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries\n",
    "\n",
    "import torch\n",
    "from transformers import RobertaTokenizer, RobertaForTokenClassification, Trainer, TrainingArguments\n",
    "from datasets import Dataset, DatasetDict\n",
    "from seqeval.metrics import classification_report as seq_classification_report\n",
    "import numpy as np\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "\n",
    "# Prepare dataset\n",
    "data = outputs\n",
    "\n",
    "dataset = Dataset.from_dict({\n",
    "    'tokens': [item['tokens'] for item in data],\n",
    "    'tags': [item['tags'] for item in data]\n",
    "})\n",
    "\n",
    "dataset_dict = dataset.train_test_split(test_size=0.2)\n",
    "\n",
    "# Model configuration\n",
    "label_names = [\"O\", \"B-Chemical\", \"B-Disease\", \"I-Disease\", \"I-Chemical\"]\n",
    "from transformers import RobertaTokenizerFast  \n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(\n",
    "    \"roberta-base\",\n",
    "    add_prefix_space=True,  # Required for word-based tokenization\n",
    "    use_fast=True\n",
    ")\n",
    "model = RobertaForTokenClassification.from_pretrained(\n",
    "    \"roberta-base\",\n",
    "    num_labels=len(label_names),\n",
    "    id2label={i: label for i, label in enumerate(label_names)},\n",
    "    label2id={label: i for i, label in enumerate(label_names)}\n",
    ")\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"],\n",
    "        truncation=True,\n",
    "        padding=False,  # Padding handled by data collator\n",
    "        is_split_into_words=True,\n",
    "        return_offsets_mapping=True\n",
    "    )\n",
    "    \n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[\"tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        \n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "            \n",
    "        labels.append(label_ids)\n",
    "    \n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "\n",
    "tokenized_data = dataset_dict.map(tokenize_and_align_labels, batched=True)\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir            = \"./results\",\n",
    "    # evaluation_strategy   = \"epoch\",     # so we actually run evaluation\n",
    "    learning_rate         = 2e-5,\n",
    "    per_device_train_batch_size = 2,\n",
    "    per_device_eval_batch_size  = 2,\n",
    "    num_train_epochs      = 3,\n",
    "    weight_decay          = 0.01,\n",
    "    logging_steps         = 50,\n",
    "    save_strategy         = \"no\",\n",
    "    report_to             = \"none\",\n",
    "    metric_for_best_model = \"entity_f1\",  # <‑‑ NEW\n",
    "    greater_is_better     = True,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Metric computation\n",
    "# --------------------------------------------------------------\n",
    "# entity‑level metrics  (BIO ➜ whole‑entity evaluation)\n",
    "# --------------------------------------------------------------\n",
    "from seqeval.metrics import (\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    classification_report,\n",
    ")\n",
    "from seqeval.scheme import IOB2\n",
    "\n",
    "ENTITY_TYPES = [\"Chemical\", \"Disease\"]     \n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "\n",
    "    true_preds, true_labels = [], []\n",
    "    for p_seq, l_seq in zip(preds, labels):\n",
    "        sent_preds, sent_labels = [], []\n",
    "        for p, l in zip(p_seq, l_seq):\n",
    "            if l == -100:        # ignore special / padding tokens\n",
    "                continue\n",
    "            sent_preds.append(label_names[p])\n",
    "            sent_labels.append(label_names[l])\n",
    "        true_preds.append(sent_preds)\n",
    "        true_labels.append(sent_labels)\n",
    "\n",
    "    ent_prec = precision_score(true_labels, true_preds, scheme=IOB2)\n",
    "    ent_rec  = recall_score(   true_labels, true_preds, scheme=IOB2)\n",
    "    ent_f1   = f1_score(       true_labels, true_preds, scheme=IOB2)\n",
    "\n",
    "    report = classification_report(\n",
    "        true_labels,\n",
    "        true_preds,\n",
    "        scheme       = IOB2,\n",
    "        output_dict  = True,\n",
    "        zero_division= 0,\n",
    "    )\n",
    "\n",
    "    metrics = {\n",
    "        \"entity_precision\": ent_prec,\n",
    "        \"entity_recall\"   : ent_rec,\n",
    "        \"entity_f1\"       : ent_f1,\n",
    "    }\n",
    "\n",
    "    # per‑entity‑type lines (Chemical, Disease, …)\n",
    "    for ent in ENTITY_TYPES:\n",
    "        if ent in report:\n",
    "            metrics[f\"{ent}_precision\"] = report[ent][\"precision\"]\n",
    "            metrics[f\"{ent}_recall\"]    = report[ent][\"recall\"]\n",
    "            metrics[f\"{ent}_f1\"]        = report[ent][\"f1-score\"]\n",
    "\n",
    "    # macro / weighted averages\n",
    "    metrics[\"macro_avg_precision\"]    = report[\"macro avg\"][\"precision\"]\n",
    "    metrics[\"macro_avg_recall\"]       = report[\"macro avg\"][\"recall\"]\n",
    "    metrics[\"macro_avg_f1\"]           = report[\"macro avg\"][\"f1-score\"]\n",
    "    metrics[\"weighted_avg_precision\"] = report[\"weighted avg\"][\"precision\"]\n",
    "    metrics[\"weighted_avg_recall\"]    = report[\"weighted avg\"][\"recall\"]\n",
    "    metrics[\"weighted_avg_f1\"]        = report[\"weighted avg\"][\"f1-score\"]\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(\n",
    "    tokenizer,\n",
    "    pad_to_multiple_of=8,\n",
    "    padding=True,\n",
    "    label_pad_token_id=-100\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_data[\"train\"],\n",
    "    eval_dataset=tokenized_data[\"test\"],\n",
    "    data_collator=data_collator,  \n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "save_directory = \"model_roberta\"\n",
    "trainer.save_model(save_directory)        \n",
    "tokenizer.save_pretrained(save_directory) \n",
    "\n",
    "\n",
    "results = trainer.evaluate()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2219199",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Entity-level (strict) F1: {results['eval_entity_f1']:.3f}\")\n",
    "print(f\"Entity-level precision : {results['eval_entity_precision']:.3f}\")\n",
    "print(f\"Entity-level recall    : {results['eval_entity_recall']:.3f}\")\n",
    "\n",
    "# Optional per‑type scores if you enabled them in compute_metrics\n",
    "for k, v in results.items():\n",
    "    if k.startswith(\"eval_\") and k.endswith(\"_f1\") and k not in {\n",
    "        \"eval_entity_f1\"}:\n",
    "        print(f\"{k.replace('eval_','').upper():>20}: {v:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3db2c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f8e0a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de766d0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979c22bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428965b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf29e1ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (rner)",
   "language": "python",
   "name": "rner"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
